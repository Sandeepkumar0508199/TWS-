Hi TDR Team,
We have successfully set up the new TWS automated trigger for Master PL Two A to B pipeline. Since this is now working correctly, we need to discontinue the existing manual trigger process to avoid duplicate runs.
Please stop triggering PL Notify CCams - this will automatically stop the chain that triggers PL TDR Request and subsequently Master PL Two A to B.
Going forward, TWS will handle the pipeline triggering automatically after EBS job completion. Let me know once you've disabled the trigger from your end.
Thanks!

My Tasks for Current Phase
Pipeline Development:

Build the main data pipeline in Azure
Connect it to our trading data sources
Set up automatic data processing
Add error handling when things go wrong

Data Processing Setup:

Configure Databricks for fraud detection
Set up the analysis notebooks
Test the alert generation system

Database Configuration:

Verify our data tables are ready
Test stored procedures
Make sure alerts save properly

Security Implementation:

Set up secure connections
Manage passwords safely
Control who can access what

Testing & Validation:

Run tests to make sure everything works
Check data quality
Verify alerts trigger correctly

My Work for September Phase
Data Integration:

Add new trading data feeds
Handle overlapping data properly
Scale up for more volume

System Monitoring:

Set up performance monitoring
Track system health
Fine-tune alert settings

What I Need to Get Started:

Azure access permissions
Database connections
Security approvals

Just checking ‚Äî since the EBS job has already run today and was marked as successful (even though it failed in ADF), can TWS run it again today to test the updated failure scenario? Or will it skip re-running because the job already triggered once?
Hi [TDR Team / Name],

Quick question regarding the EBS job behavior:

What happens if there is no file received from upstream? Does the EBS job fail, show as completed, or stay in progress?

If a file is received but it has no data (empty file), will the EBS job still show as successful, and will it still trigger the analytics pipeline?

Just trying to understand how the job behaves in these scenarios for proper handling on our side. Let me know when you get a chance. Thanks!

How often does the EBS file get delayed?

Is it a frequent issue or only occasional?

By what time is the file typically received?

If the file is delayed and processed later in the day:

Will the TDR team inform us once it‚Äôs successfully processed?

Should we plan to manually rerun only the impacted T-1 analytics jobs (e.g., spoofing) afterward?

Does this approach sound reasonable to you?

If the EBS job hasn‚Äôt completed by 10:00 AM, we‚Äôll go ahead and trigger the analytics pipeline manually.

Later, we‚Äôll rerun only the impacted T-1 pipelines once the file is received and processed.

Let us know your thoughts. Thanks again!


Absolutely! Here's a **refined and validated list of questions** after cross-checking with:

* üó£Ô∏è Your **meeting notes**
* üì∏ Your **screenshot**
* ‚úÖ Logical consistency and clarity for stakeholders (TDR team, ops, etc.)

Each question is **grouped by theme** and verified to be **relevant, non-redundant**, and **actionable**.

---

## ‚úÖ Final Cross-Checked & Validated EBS Job Behavior Questions

---

### üîπ 1. Upstream File Behavior

1. What happens if there is **no file received** from upstream?

   * Does the EBS job **fail**, remain **in progress**, or **show as completed**?

2. If a file is received but contains **no data** (i.e., an empty file):

   * Will the EBS job still **show as successful**?
   * Will this **trigger the analytics pipeline**?

3. If the file is only **partially received** (incomplete data):

   * Will the job still show as **Succeeded**?
   * Is there any mechanism to **flag partial loads**?

4. How is the **EBS job status reflected** in the `TDRLoadStatusView`?

   * Does it change only upon **completion**, or also after **upstream validation**?

---

### üîπ 2. Delay & Timeliness

5. How often does the **EBS file get delayed**?

6. Is EBS file delay a **frequent issue** or only **occasional**?

7. By what **time is the file typically received** on successful days?

8. Are there **specific upstream sources** responsible for delays more often?

9. Can we get **historical stats** on how many times the EBS job was:

   * Delayed
   * Failed
   * Took longer than expected to complete?

---

### üîπ 3. Notification & Coordination

10. If the file is delayed and processed later:

* Will the **TDR team notify us** once it has been successfully processed?

11. Can we get a **standardized status or flag** to distinguish:

* No file received
* File received but empty
* File received partially
* Job failed

12. If the EBS job completes successfully, but `LoadStatus` is **not updated** in time:

* Should we wait, or is there a **secondary confirmation mechanism**?

---

### üîπ 4. Triggering the Analytics Pipeline

13. If the EBS job hasn‚Äôt completed by **10:00 AM**:

* Can we go ahead and **trigger the analytics pipeline manually** to avoid blocking others?

14. Can we define a **fallback trigger time** (e.g., 10:00 AM) to bypass dependency checks and allow partial pipeline execution?

15. Should **TWS or ADF** include a **retry/polling mechanism** to check EBS job status every few minutes between 9:00‚Äì10:00 AM?

---

### üîπ 5. Manual Rerun & Support

16. Once the EBS job completes **after the fallback time**:

* Should we **manually rerun only the impacted T-1 analytics jobs** (e.g., spoofing FX)?

17. Should we create a **clear SOP or playbook** for production support:

* When EBS is delayed or fails
* How to rerun affected pipelines
* Who is responsible

18. Can we document an **alerting mechanism** for production support to notify teams when manual rerun is needed?

---

### üîπ 6. LoadStatus Field Reliability

19. Is the `LoadStatus='Succeeded'` value in `TDRLoadStatusView` always a **reliable trigger** for downstream jobs?

20. What are the **possible values** for `LoadStatus`, and what do they each **indicate**?

------------------------------------------------------------------------------------------------------------------------------------------------

Let me know if you'd like these added to a **Google Doc**, **Confluence page**, or formatted into an **email** for sharing with your team or TDR stakeholders.
If the EBS file is only partially received, will the job still show as successful?

Is there any way to detect or flag when a partial file has been processed?

How is the EBS job status reflected in the TDR load status view? Does it update only after completion, or does it account for upstream validation as well?

How often do we see delays with the EBS file? Is this something that happens regularly or just once in a while?

On days when everything runs smoothly, by what time is the EBS file usually received and processed?

Are there any particular upstream systems that tend to cause delays more often than others?

If the file gets delayed but is eventually processed, will someone from the TDR team notify us when it's ready?

Can we get a clear status indicator that tells us whether the file was empty, partial, or failed ‚Äî so we know how to handle it downstream?

If the EBS job completes successfully but the load status hasn‚Äôt been updated in time, should we wait for it, or is there a backup way to confirm that it‚Äôs done?

If the EBS job still hasn‚Äôt completed by 10:00 AM, should we go ahead and trigger the analytics pipeline anyway to avoid blocking everything else?

Would it make sense to set a fallback time ‚Äî like 10:00 AM ‚Äî to automatically trigger the analytics pipeline if the EBS job hasn‚Äôt finished?

Instead of a fixed time, can we add a polling mechanism that checks the EBS job status every few minutes between 9:00 and 10:00 AM?

If the EBS job completes after our fallback time, is it okay to manually rerun just the affected pipelines ‚Äî like spoofing FX ‚Äî instead of the full batch?

Do we need to define a clear SOP for production support in case the EBS job fails or is delayed? Who should be responsible for triggering reruns in those cases?


